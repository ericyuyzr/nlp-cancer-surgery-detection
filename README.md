# Cancer Surgery Classification using BERT and Random Forest

This project uses BERT embeddings and a Random Forest classifier to predict surgery-related prodecures from SPR ha_procedure_description.

---

### ðŸš€ Project Overview

- Input: Clinical procedure descriptions (`ha_procedure_description`)
- Model: BERT (Bio_ClinicalBERT) + PCA + Random Forest
- Goal: Classify surgeries into binary classes
- Imbalanced data handled using undersampling (during training only)

The SPR database is an important data source, containing surgical records from across British Columbia. However, reviewing new incoming procedures and identifying cancer procedures each month is a manual and time-consuming process.

With the growing power and popularity of large language models (LLMs), weâ€™re exploring whether these tools can help automate this review process and save valuable time.


### ðŸ§± Project Structure
```
nlp-cancer-surgery-detection/
â”œâ”€â”€ data/ # Input and output CSV files
â”œâ”€â”€ models/ # Saved pipeline and components
â”œâ”€â”€ notebooks/ # Saved data explore, baseline model performance and model selection
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ train_pipeline.py # Training script
â”‚ â””â”€â”€ inference_model.py # Inference script
â”œâ”€â”€ environment.yml
â””â”€â”€ README.md
```

### ðŸ”¬ Approach

#### 1. **Baseline Model**
I used `facebook/bart-large-mnli` with zero-shot inference as the baseline model. The idea was that if it performed well, there would be no need to explore alternative methods.

I chose **recall** as the evaluation metric because it's critical not to miss any cancer-related procedures. Since the modelâ€™s output will still be reviewed by humans, we can tolerate a reasonable number of false positives (i.e., non-cancer procedures being included).

However, the results were disappointing. The **recall for the positive (cancer) group was only 0.26** on the training dataset â€” correctly identifying just **251 out of 981** cancer-related procedures.

This outcome is somewhat expected, as the model was not trained on clinical text, and the procedure descriptions in our dataset are typically very concise.

Given that we do not have access to a GPU, fine-tuning large language models (LLMs) is not feasible. Therefore, my strategy is to leverage the embeddings generated by pre-trained LLMs as input features for a simpler machine learning model. This allows us to benefit from the rich representations captured by LLMs without the computational cost of fine-tuning.


#### 2. **Text Preprocessing**
- Used **SciSpaCy's `en_core_sci_sm`** model (`spacy.load("en_core_sci_sm")`) to preprocess biomedical text.
- Applied lemmatization, stopword removal, and punctuation filtering.
- This step helped standardize clinical terminology before generating embeddings.
- Later, I found that this step seems redundant. We plan to use **Bio_ClinicalBERT**, which can handle natural language directly without requiring lemmatization, stopword removal, or punctuation filtering. However, I will leave this step in the current pipeline for now and remove it later to observe the impact on the results.

#### 3. **Embedding with BERT**
- Used the pretrained **Bio_ClinicalBERT** model to generate contextual embeddings of procedure descriptions. This model is trained with clinical notes
- Extracted the `[CLS]` token representation for downstream classification.

#### 4. **Dimensionality Reduction**
- Applied **Principal Component Analysis (PCA)** to reduce the high-dimensional BERT embeddings. Since the dimension of `[CLS]` is 768 which is too big for our downstream models and also the data size we have.
- After review the cumulative explained variance plot, I decided to keep 300 components

#### 5. **Handling Imbalanced Data**
- The dataset was highly imbalanced (â‰ˆ 97% vs 3% class distribution).
- Used **random undersampling** during training to balance classes while keeping the inference pipeline clean.
- I also tried other sampling methods such as oversampling and synthetic sampling, but undersampling proved to be the most effective. This was expected. Both oversampling and synthetic sampling rely on PCA components derived from BERT embeddings. Since the high-dimensional vector representations of the text are complex, simply adding synthetic data can easily distort those representations.

#### 6. **Model Selection**
- Trained and evaluated two models; **Random Forest** performed best in terms of recall.
- Used cross-validation and grid search for hyperparameter tuning.


### ðŸ“Š Results
At the very first step, I set aside 20% of procedures from each cancer type, as well as non-cancer procedures, to form the test dataset.

After training and hyperparameter tuning, the model achieved a **recall of 0.97** for the positive (cancer) group when using a threshold of 0.4 â€” meaning only **5 cancer procedures were misclassified**. 

For the negative (non-cancer) group, the recall was **0.48**, indicating that the model helped us ignore roughly half of the non-cancer procedures. However, the remaining half still require manual review.

One of the disadvantages of this model is its lack of interpretability. Since it relies on BERT embeddings, it is difficult to understand the reasoning behind the misclassified cases, especially for the false negatives and false positives. Despite the limitations, there were some interesting findings. The model tends to classify resection-related procedures as cancer-related. This makes sense, as resections are commonly associated with cancer treatment. However, not all resection procedures are actually cancer-related, which contributes to some of the false positives.

### ðŸš€ Next Steps

- Maybe no need to use **SciSpaCy's `en_core_sci_sm`** .
- Try to train a single layer of classfier instead of using Random Forest. Since we already use the output from BERT, it's natural to still use deep learning airchtechures. 
- Even for Colleen and me, many procedures are quite difficult to classify during manual review. It's often challenging to make a clear decision based solely on the procedure descriptions. In practice, we also consider additional information such as diagnosis and other info. This suggests that incorporating more contextual text data into the model â€” like diagnosis information â€” could potentially help reduce the number of false positives.